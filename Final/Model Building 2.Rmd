---
title: "Model Building2"
output: pdf_document
date: "2023-12-08"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(include = FALSE)
options(repos = c(CRAN = "https://cloud.r-project.org"))
library(modelr)
library(tidyverse)
library(ggplot2)
library(GGally)

library(dplyr)
#install.packages("lmtest")
library(lmtest)
#install.packages(c("MASS", "car", "caret"))
library(MASS)
library(car)
library(caret)


```

1) The first question I'm trying to answer is how well Unemployment, labor force statistics, and population can predict certain types of crime in Maryland. For this PCA I removed all crime statistics because those are my dependent values. Last time I included them because I misunderstood the process. I also scaled the data for this new PCA to get more accurate results. Furthermore based on the suggestions from the last model building assignment I added a bi plot to display the variable relationships. As well as creating a regression model for seven different types of crime statistics. In the end I discovered that the pca could predict total crime better than all other crimes besides larceny theft. Larceny theft was the only crime that the model could predict better than total crime. I came to this conclusion by comparing r scores for the models which measures how much variability it was able to explain. Larceny had an r score of .8165 whereas total crime was .7282. Thus larceny prediction will be more accurate in general. 

2) My second question was about the influence population groups had on a models ability to predict crime based on unemployment and Labor force. The variables used in my second model are Index_Count, Population, Unemployment, and Labor. Index_Count represents the amount of crimes recorded in a year for a particular county. For my second model after reading the advice from the previous assignment I decided I would go with a glm model. After some research I first went with a poisson regression model. I included the model below for reference. When I saw the results I realized that a poisson model was not the best method because of the amount of over dispersion present in my data. So after more research I settled on a negative binomial regression because they are able to fit data better when the variance is higher than the mean which was definitely the case. The difference was staggering between the models. The second model did the trick and most residual deviance were only off my 20 or so from the degrees of freedom. Compared to the poisson where there was a difference of tens of thousands. In conclusion as the I went through the population groups the more people in a population group increased the AIC. Which is the score of a models goodness of fit and simplicity.  Thus population does affect the ability to accurate predict crime based on unemployment and labor force because it affects the how well the model fits to the data and thus affecting prediction accuracy.

3) For my third I question I'm trying to figure out if a models is able to predict crime more accurately if the data is aggregated rather than if I train based on crime in each county. In the previous assignment I attempted to answer whether unemployment and labor pct could predict certain types of crime better than others. But it was too similar to what I did with my first question so I decided to do something different. I decided to go with a negative binominal regression and used the New York crime data set to answer my question. After using these models I came to the conclusion that aggregating crime data provided more accurate results for the state rather than a county by county basis was more accurate. I came to this conclusion by comparing the AIC scores of the two regressions. Aggregate AIC was 841.17 and non aggregate AIC was 37436. This shows clearly that aggregating data is a better method for predicting crime with unemployment and labor force pct.


#1st Project Question
####Preprocessing Data for PCA
```{r include=FALSE}
setwd("X:/Coding/Rstudio/R Projects/Data 490 Model Building/Documentation")
yearly_crime_m_raw <- read.csv("Maryland crime 1975-2022.csv") %>%

  rename(Year = YEAR)

crimeTotalm_raw <- yearly_crime_m_raw %>%
  
  group_by(Year) %>%
  
  mutate(theft_total = sum(LARCENY.THEFT)) %>%

  mutate(murder_total = sum(MURDER)) %>%
  
  mutate(be_total = sum(B...E)) %>%
  
  filter(Year != 1975)

#Maryland Unemployment
mu <- read.csv("MDUR.csv") %>%
  
  #Converts the Date Column from a Date to a Year
  mutate(DATE = year(DATE)) %>%
  
  #Changes the Date Column name to Year
  rename(Year = DATE) %>%
  
  #Rename column to Unemployment
  rename(Unemployment = MDUR)


#Remove 2023 Which is NA
mu <- mu %>%
  filter(Year != 2023)


#Convert column from character to numeric
mu$Unemployment <- as.numeric(mu$Unemployment)


#Round data to second decimal
mu$Unemployment <- round(mu$Unemployment, digits = 2)

#Maryland Labor Force
ml <- read.csv("maryland_labor.csv") %>%
  
  #Converts the Date Column from a Date to a Year
  mutate(DATE = year(DATE)) %>%
  
  #Changes the Date Column name to Year
  rename(Year = DATE) %>%
  
  #Rename column to Labor
  rename(Labor = LBSSA24) 



#Remove 2023 Which is NA
ml <- ml %>%
  filter(Year != 2023)


#Convert column from character to numeric
ml$Labor <- as.numeric(ml$Labor)



#Round data to second decimal
ml$Labor <- round(ml$Labor, digits = 2)

#Maryland Crime Only Year and Grand Total
crimeTotalm <- crimeTotalm_raw[, c(1:3,11,39:41)]

#Merge Unemployment with Yearly crime
crimeTotalm <- merge(x = mu, y = crimeTotalm, by = "Year", all.y = TRUE)

#Merge Labor force with Unemployment and Yearly Crime
crimeTotalm <- merge(x = ml, y = crimeTotalm, by = "Year",
all.y = TRUE)


countylist <- unique(crimeTotalm$JURISDICTION)

# Create an empty data frame to store results
correlation_data <- data.frame(JURISDICTION = character(),
                               correlation = numeric(),
                               average_population = numeric(),
                               stringsAsFactors = FALSE)

for (i in seq_along(countylist)) {
  county_data <- crimeTotalm %>%
    filter(JURISDICTION == countylist[i])
  
  # Calculate correlation
  correlation_value <- cor(county_data$Labor, county_data$GRAND.TOTAL)
  
  # Calculate average population
  average_population <- mean(county_data$POPULATION)
  
  # Append the results to the correlation_data data frame
  correlation_data <- bind_rows(correlation_data,
                                 data.frame(JURISDICTION = countylist[i],
                                            correlation = correlation_value,
                                            average_population = average_population))
}

#New York Unemployment
nu <- read.csv("NYUR.csv")%>%
  
  #Converts the Date Column from a Date to a Year
  mutate(DATE = year(DATE)) %>%
  
  #Changes the Date Column name to Year
  rename(Year = DATE) %>%
  
  #Rename column to Unemployment
  rename(Unemployment = NYUR)

#Remove 2023 Which is NA
nu <- nu %>%
    filter(Year != 2023)


#Convert column from character to numeric
nu$Unemployment <- as.numeric(nu$Unemployment)



#Round data to second decimal
nu$Unemployment <- round(nu$Unemployment, digits = 2)

#New York Labor Force
nl <- read.csv("newyork_labor.csv") %>%
  
  #Converts the Date Column from a Date to a Year
  mutate(DATE = year(DATE)) %>%
  
  #Changes the Date Column name to Year
  rename(Year = DATE) %>%
  
  #Rename column to Labor
  rename(Labor = LBSSA36) 

#Remove 2023 Which is NA
nl <- nl %>%
    filter(Year != 2023)

#Convert column from character to numeric
nl$Labor <- as.numeric(nl$Labor)

#Round data to second decimal
nl$Labor <- round(nl$Labor, digits = 2)

#Washington Unemployment
wu <- read.csv("WAUR.csv")%>%
  
  #Converts the Date Column from a Date to a Year
  mutate(DATE = year(DATE)) %>%
  
  #Changes the Date Column name to Year
  rename(Year = DATE) %>%
  
  #Rename column to Unemployment
  rename(Unemployment = WAUR)


#Remove 2023 Which is NA
wu <- wu %>%
    filter(Year != 2023)

#Convert column from character to numeric
wu$Unemployment <- as.numeric(wu$Unemployment)



#Round data to second decimal
wu$Unemployment <- round(wu$Unemployment, digits = 2)

#Washington Labor Force
wl <- read.csv("washington_labor.csv") %>%
  
  #Converts the Date Column from a Date to a Year
  mutate(DATE = year(DATE)) %>%
  
  #Changes the Date Column name to Year
  rename(Year = DATE) %>%
  
  #Rename column to Labor
  rename(Labor = LBSSA53) 

#Remove 2023 Which is NA
wl <- wl %>%
   filter(Year != 2023)


#Convert column from character to numeric
wl$Labor <- as.numeric(wl$Labor)

#Round data to second decimal
wl$Labor <- round(wl$Labor, digits = 2)





#Washington
#Washington crime that only leaves state totals and renames columns
yearly_crime_w_raw <- read.csv("washington_state_crime.csv") %>% 
  
  rename(Year = year)

yearly_crime_w <- yearly_crime_w_raw %>%
  
  group_by(Year) %>%
  
  filter(county == "STATE") 


#Create data set the contains all Data from the SRS reporting system 1990-2011

yearly_crime_w_srs <- yearly_crime_w[c(1:22),c(1,3,22)] %>%
    
  rename(yearly_crime = SRS_TOTAL)

#Create data set that contains all Data from the NIBRS system starting in 2012-2020  

yearly_crime_w_nib <- yearly_crime_w[c(23:33),c(1,3,46)] %>%
    
rename(yearly_crime = NIB_TOTAL)

#Combine both Datasets
yearly_crime_w_clean <- rbind.data.frame(yearly_crime_w_nib,yearly_crime_w_srs)

#Order Data set by Year
yearly_crime_w_clean <- yearly_crime_w_clean[order(yearly_crime_w_clean$Year), ]


  #Merge Unemployment with Yearly crime
washington <- merge(x = wu, y = yearly_crime_w_clean, by = "Year", all.y = TRUE)


#Merge Labor force with Unemployment and Yearly Crime
washington <- merge(x = wl, y = washington, by = "Year",
all.y = TRUE)

#New York Crime w/ Grand Total
yearly_crime_n_raw <- read.csv("new york crime.csv")

yearly_crime_n <- yearly_crime_n_raw %>%
  
  group_by(Year) %>%
  
  mutate(yearly_crime = sum(Index_Count)) %>%
  
  mutate(yearly_pop = sum(Population))
  

#New York Crime Only Year and Grand Total
yearly_crime_n_clean <- yearly_crime_n[c(1:33), c(2,12, 13)]



#Merge Unemployment with Yearly crime
newyork <- merge(x = nu, y = yearly_crime_n_clean, by = "Year", all.y = TRUE)

#Merge Labor force with Unemployment and Yearly Crime
newyork <- merge(x = nl, y = newyork, by = "Year",
all.y = TRUE)

newyork_noagg <- merge(x = nu, y = yearly_crime_n_raw, by = "Year", all.y = TRUE)

newyork_noagg <- merge(x = nl, y = newyork_noagg, by = "Year", all.y = TRUE)

```

Maryland PCA
```{r}

#install.packages("arm")
library(arm)


pca_crime <- crimeTotalm_raw

pca_crime <- merge(x = mu, y = pca_crime, by = "Year", all.y = TRUE)

pca_crime <- merge(x = ml, y = pca_crime, by = "Year", all.y = TRUE)

selected_columns5 <- pca_crime[, c("POPULATION","Labor","Unemployment")]

scaled_data <- scale(selected_columns5)


pca_results <- prcomp(scaled_data, center = TRUE, scale. = TRUE)

principal_components <- pca_results$x



biplot(pca_results)


#1
print("Overall Crime")

regression_crime <- data.frame(principal_components, grand.total = pca_crime$GRAND.TOTAL)

lm_model <- lm(grand.total ~ ., data = regression_crime)

summary(lm_model)

nb_model <- glm.nb(grand.total ~ ., data = regression_crime)

summary(nb_model)



#2
print("Breaking and Entering")
regression_crime <- data.frame(principal_components, grand.total = pca_crime$B...E)

lm_model <- lm(grand.total ~ ., data = regression_crime)

summary(lm_model)

#3
print("Larceny Theft")
regression_crime <- data.frame(principal_components, grand.total = pca_crime$LARCENY.THEFT)

lm_model <- lm(grand.total ~ ., data = regression_crime)

summary(lm_model)

#4
print("Robbery")
regression_crime <- data.frame(principal_components, grand.total = pca_crime$ROBBERY)

lm_model <- lm(grand.total ~ ., data = regression_crime)

summary(lm_model)

#5
print("Murder")
regression_crime <- data.frame(principal_components, grand.total = pca_crime$MURDER)

lm_model <- lm(grand.total ~ ., data = regression_crime)

summary(lm_model)

#6
print("Aggrevated Assualt")
regression_crime <- data.frame(principal_components, grand.total = pca_crime$AGG..ASSAULT)

lm_model <- lm(grand.total ~ ., data = regression_crime)

summary(lm_model)

#7
print("MV THEFT")
regression_crime <- data.frame(principal_components, grand.total = pca_crime$M.V.THEFT)

lm_model <- lm(grand.total ~ ., data = regression_crime)

summary(lm_model)
```

```{r}
#3
print("Larceny Theft")
regression_crime <- data.frame(principal_components, grand.total = pca_crime$LARCENY.THEFT)

lm_model <- lm(grand.total ~ ., data = regression_crime)

summary(lm_model)

#4
print("Robbery")
regression_crime <- data.frame(principal_components, grand.total = pca_crime$ROBBERY)

lm_model <- lm(grand.total ~ ., data = regression_crime)

summary(lm_model)
              
```          

   
#2nd Project Question
####NewYork Preprocessing
```{r include=FALSE}

# New York
setwd("X:/Coding/Rstudio/R Projects/Data 490 Model Building/Documentation")
#New York Crime w/ Grand Total
yearly_crime_n_raw <- read.csv("new york crime.csv")

yearly_crime_n <- yearly_crime_n_raw %>%
  
  group_by(Year) %>%
  
  mutate(yearly_crime = sum(Index_Count))


#New York Unemployment
nu <- read.csv("NYUR.csv")%>%
  
  #Converts the Date Column from a Date to a Year
  mutate(DATE = year(DATE)) %>%
  
  #Changes the Date Column name to Year
  rename(Year = DATE) %>%
  
  #Rename column to Unemployment
  rename(Unemployment = NYUR)

#Remove 2023 Which is NA
nu <- nu %>%
    filter(Year != 2023)


#Convert column from character to numeric
nu$Unemployment <- as.numeric(nu$Unemployment)



#Round data to second decimal
nu$Unemployment <- round(nu$Unemployment, digits = 2)

#New York Labor Force
nl <- read.csv("newyork_labor.csv") %>%
  
  #Converts the Date Column from a Date to a Year
  mutate(DATE = year(DATE)) %>%
  
  #Changes the Date Column name to Year
  rename(Year = DATE) %>%
  
  #Rename column to Labor
  rename(Labor = LBSSA36) 

#Remove 2023 Which is NA
nl <- nl %>%
    filter(Year != 2023)

#Convert column from character to numeric
nl$Labor <- as.numeric(nl$Labor)

#Round data to second decimal
nl$Labor <- round(nl$Labor, digits = 2)


# Left join 'yearly_crime_n_raw' with 'nu' based on the 'Year' column
yearly_crime_n_raw <- left_join(yearly_crime_n_raw, nl, by = 'Year')

# Left join 'yearly_crime_n_raw' with 'nu' based on the 'Year' column
yearly_crime_n_raw <- left_join(yearly_crime_n_raw, nu, by = 'Year')
```


pop group prep
```{r}


results <- yearly_crime_n_raw %>%
  group_by(County) %>%
  summarize(mean_population = mean(Population)) %>%
  arrange(desc(mean_population))



summary(results)


#Breaks the the counties into four groups 
for( i in seq_along(results$mean_population)) {
  if (results$mean_population[i] > 232099) {
    
    results$pop_group[i] = 4
    
  }else if(results$mean_population[i] > 91082){
    
    results$pop_group[i] = 3
    
  }else if(results$mean_population[i] > 50689){
    
    results$pop_group[i] = 2
    
  }else if(results$mean_population[i] <= 50689 ){
    
    results$pop_group[i] = 1}
}

# Merge the two data frames on the 'County' column
merged_data <- merge(yearly_crime_n_raw, results[, c('County', 'pop_group')], by = 'County', all.x = TRUE)

# Print the merged data frame




```


poisson model 2
```{r}
pop_groups <- unique(merged_data$pop_group)

# Create an empty list to store regression models
regression_models <- list()

# Iterate through each population group and fit a Poisson regression model
for (group in pop_groups) {
  subset_data <- filter(merged_data, pop_group == group)
  model <- glm(Index_Count ~ Unemployment + Labor + Population, data = subset_data, family = poisson(link = "log"))
  regression_models[[group]] <- model
  
  # Print summary for each model with the corresponding group
  cat("Summary for", group, ":\n")
  print(summary(model))
  cat("\n")
}

count_by_pop_group <- table(results$pop_group)
print(count_by_pop_group)

```

negative binomial
```{r}
pop_groups <- unique(merged_data$pop_group)

# Create an empty list to store regression models
regression_models <- list()

# Iterate through each population group and fit a Negative Binomial regression model
for (group in pop_groups) {
  subset_data <- filter(merged_data, pop_group == group)
  
  # Fit Negative Binomial Regression
  model <- glm.nb(Index_Count ~ Unemployment + Labor + Population, data = subset_data)
  regression_models[[group]] <- model
  
  # Print summary for each model with the corresponding group
  cat("Summary for", group, ":\n")
  print(summary(model))
  cat("\n")
}

# (Assuming 'results' is the correct dataframe to use for count_by_pop_group)
count_by_pop_group <- table(results$pop_group)
print(count_by_pop_group)

```


#3rd Project question
```{r}

ny_agg <- glm.nb(yearly_crime ~ Unemployment + Labor + yearly_pop , data = newyork)

ny_noagg <- glm.nb(Index_Count ~ Unemployment + Labor + Population, data = newyork_noagg)

summary(ny_agg)

summary(ny_noagg)


```



```{r}

options(scipen = 999)
set.seed(123)
train_indices <- createDataPartition(newyork_noagg$Index_Count, p = 0.8, list = FALSE)
train_data <- newyork_noagg[train_indices, ]
test_data <- newyork_noagg[-train_indices, ]

# Fit the negative binomial regression model
nb_model <- glm.nb(Index_Count ~ Unemployment + Labor + Population, data = train_data)

# Make predictions on the testing set
predictions <- predict(nb_model, newdata = test_data, type = "response")

# Evaluate model performance
performance_metrics <- postResample(predictions, test_data$Index_Count)

# Print performance metrics (you can choose different metrics)
print(performance_metrics)

# Create a visualization
png("your_plot_noagg.png", width = 800, height = 600)

plot(predictions, test_data$Index_Count, main = "Predicted vs. Actual",
     xlab = "Predicted Values", ylab = "Actual Values")  # Set x-axis limit
    

abline(a = 0, b = 1, col = "red")  # Diagonal line for reference
dev.off()

```






```{r}

options(scipen = 999)
set.seed(123)
train_indices <- createDataPartition(new_york$Index_Count, p = 1, list = FALSE)
train_data <- new_york[train_indices, ]
test_data <- newyork_noagg[-train_indices, ]

# Fit the negative binomial regression model
nb_model <- glm.nb(Index_Count ~ Unemployment + Labor + Population, data = train_data)

# Make predictions on the testing set
predictions <- predict(nb_model, newdata = test_data, type = "response")

# Evaluate model performance
performance_metrics <- postResample(predictions, test_data$Index_Count)

# Print performance metrics (you can choose different metrics)
print(performance_metrics)

# Create a visualization
png("your_plotagg1.png", width = 800, height = 600)

plot(predictions, test_data$Index_Count, main = "Predicted vs. Actual",
     xlab = "Predicted Values", ylab = "Actual Values")  # Set x-axis limit
    

abline(a = 0, b = 1, col = "red")  # Diagonal line for reference
dev.off()


```




















